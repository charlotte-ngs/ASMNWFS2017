---
title: "Angewandte Statistische Methoden in den Nutztierwissenschaften"
author: "Peter von Rohr"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: ["ASMNW.bib"]
biblio-style: apalike
link-citations: yes
description: "Unterlagen zur Vorlesung Angewandte Statistische Methoden in den Nutztierwissenschaften."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::knit_hooks$set(conv.odg = rmddochelper::odg.graphics.conv.hook)
r6objTableAbbrev <- rmddochelper::R6ClassTableAbbrev$new()
```

# Vorwort {-}
Dieses Dokument umfasst die kompletten Unterlagen zur Vorlesung __Angewandte Statistische Methoden in den Nutztierwissenschaften__. Der Titel dieser Vorlesung ist sehr allgemein gehalten. Dies würde es erlauben einen grosszügigen Überblick über eine breite Palette an statistischen Methoden, welche in den Nutztierwissenschaften eingesetzt werden, zu geben. 

Wir schlagen an dieser Stelle aber einen anderen Weg ein, und fokussieren uns auf die statistischen Methoden in der genomischen Selektion. Nur diese bewusste Wahl eines spezifischen Gebietes ermöglicht es uns, den behandelten Stoff angemessen zu vertiefen. Im anschliessenden Unterabschnitt wollen wir die hier getroffene Entscheidung der Fokusierung auf die genomische Selektion motivieren. Dabei wird klar, dass wir mit der Wahl des Themas der multiplen linearen Regression als Ausgangspunkt auch eine Leserschaft ansprechen, welche nicht primär an der Tierzucht interessiert ist.


## Motivation {-}
Vom Standpunkt der statistischen Modellierung, ist das einfache lineare Modell mit fixen Effektstufen für den Einsatz in der genomischen Selektion ausreichend. Diese Art von Modellen werden auch als Regressionsmodelle bezeichnet. Die Problematik entsteht erst bei der Technik, welche wir für die Schätzung der unbekannten Parameter verwenden können. In der klassischen Regressionsanalyse ist die Methode der kleinsten Quadrate (Least Squares) die Methode der Wahl. Least Squares können wir aber für die genomische Selektion nicht verwenden, da die Anzahl unbekannter Parameter ($p$) grösser ist als die Anzahl Beobachtungen ($n$). 

Mit der steigenden Grösse und Komplexität von aktuellen Datensätzen tritt das soeben beschriebene Problem nicht nur in der Tierzucht auf, sondern es gibt eine breite Palette von Anwendungen. In der Vorlesung beschrieben wir diese Problematik am Beispiel der genomischen Selektion und es werden alternative Techniken zur Schätzung von Parametern vorgeschlagen. Da die Methode der multiplen Regressionsanalyse in früheren Vorlesungen behandelt wurde, bietet diese ein idealer Ausgangspunkt für den in dieser Veranstaltung präsentierten Stoffinhalt.


## Einordnung {-}
Die Vorlesung __Angewandte Statistische Methoden in den Nutztierwissenschaften__ ist eine halb-semestrige Veranstaltung und wird im Masterstudiengang Agrarwissenschaften der ETH Zürich angeboten.


## Lernziele {-}
Für die Verwendung des hier präsentierten Stoffs schlagen wir die folgenden Lernziele vor. 

Die Studierenden ...

- kennen die Eigenschaften der multiplen linearen Regression und 
- können einfache Datensätze mithilfe der Regressionsmethode analysieren 
- wissen wieso multiple lineare Regressionen bei der genomischen Selektion nicht brauchbar ist 
- kennen die in der genomischen Selektion verwendeten statistischen Verfahren, wie
    + BLUP-basierte Verfahren, 
    + Bayes'sche Verfahren und 
    + die LASSO Methode 
- können einfache Übungsbeispiele mit der Statistiksoftware R erfolgreich bearbeiten.


<!--chapter:end:index.Rmd-->

# Einführung {#intro}

Den in dieser Vorlesung präsentierte Stoff kann aus mehreren Gesichtspunkten betrachtet werden. Aus Sicht der Tierzucht behandeln wir die statistischen Methoden, welche in der __genomischen Selektion__ angewendet werden. Für Statistiker stellen wir verschiedene Methoden der Regularisierung in hoch-parametrischen Modellen vor. In der sehr populären Disziplin des __Machine Learnings__ wird das hier besprochene Problem als die Selektion von relevanten Features im Kontext des Supervised Learnings dargestellt.


## Beschreibung des Problems {#problem}
Alle die soeben genannten Formulierungen beschreiben das gleiche Problem. Wir gehen von einem Datensatz aus, welcher aus Beobachtungen besteht. Jede Beobachtung ist charakterisiert durch sehr viele unabhängige Grössen. Die Gewichtung der zu einer Beobachtung gehörenden Grössen wird über unbekannte Parameter erreicht. 

Als Beispiel für einen solchen Datensatz können wir eine Population mit SNP-typisierten Tieren betrachten. Das Typisierrungsergebnis für ein bestimmtes Tier enthält die Genotypen an den Genorten, welche bei der Typisierung untersucht werden. Die einzelnen Genorte werden als sogenannte Single Nucleotide Polymorphisms (SNP) bezeichnet. In Abhängigkeit des anbietenden Labors gibt es verschiedene Optionen für die gewünschte Typisierung. Die Optionen unterscheiden sich vor allem in der Dichte der untersuchten Genorte. Das heisst bei einer grösseren Dichte werden mehr SNPs untersucht. Typische Werte von gängigen Anbietern bewegen sich im Bereich zwischen 50000 (50K) bis rund 800000 (800K) untersuchte SNPs pro untersuchtes Genom. Die totale Anzahl an SNP im Genom beträgt rund 20 Millionen. Somit ist ein Typisierungsergebnis eine vom Anbieter gemachte Auswahl aller verfügbaren SNPs.


## Rückblick {#background}
Bis Anfangs des 21. Jahrhundert wurden eigentlich keine genomischen Informationen in Zuchtprogrammen berücksichtigt. Mit genomischer Information ist hier die Genotyp-Varianten einer grosser Anzahl von Genorten, welche über das ganze Genom verteilt ist. Um die Jahrtausendwende waren sehr viele ForscherInnen in einem Gebiet aktiv, welches damals als Mapping von sogenannten `r r6objTableAbbrev$add_abbrev(psAbbrev = "QTL", psMeaning = "Quatitative Trait Loci")` bezeichnet wurde. Eine Übersicht zu QTL ist im Buch [@BBC2008]. Das Ziel der Untersuchungen im Bereich QTL-Mapping war das Finden von Regionen im Genom, welche wichtig sind für die Ausprägung von spezifischen Phänotypen. Heute spricht man nicht mehr QTL-Mapping sondern heute wird die Suche von genetischen Orten, welche einen wichtigen Einfluss auf die Ausprägung eines Phänotyps haben, mit `r r6objTableAbbrev$add_abbrev(psAbbrev = "GWAS", psMeaning = "Genome Wide Association Study")` bezeichnet. 

Trotz umfangreicher Forschungstätigkeit auf dem Gebiet des QTL-Mappings, fanden keine Resultate aus diesen Arbeiten den Weg in die praktische Zuchtarbeit. Somit verläuft die Zuchtarbeit bis vor kurzem nach dem klassischen Schema, welches nachfolgend gezeigt ist.

```{r ZuchtprogrammKomplett, conv.odg=TRUE, odg.path="odg", odg.graph.cache=TRUE, fig.align='center', echo=FALSE, results='asis'}
knitr::include_graphics(path = "ZuchtprogrammKomplett.pdf")
```

### Paradigmenwechsel
Die Publikation [@MHG2001] gilt als Grundstein für eine neue Ära in der praktischen Zuchtarbeit. Die Autoren haben gezeigt, wie genomische Information, welche in genügender Dichte vorliegen muss, zur Schätzung von Zuchtwerten verwendet werden kann. Sie konnten auch statistische Methoden zeigen, mit welchen die Parameter in verwendeten Modell geschätzt werden können. Wir werden zu einem späteren Zeitpunkt noch genauer auf den Inhalt des Papers von [@MHG2001] zurückkommen.


### Vor der genomischen Selektion
Von Anfangs der 1980-er Jahre wurden die statistischen Auswertungen in den Zuchtprogrammen auf das BLUP-Tiermodell abgestellt. In dieser Zeit wurden die einfachen Modelle auch durch verschiedene Erweiterungen ausgebaut. Bei der Milchproduktion wurde von einfachen Laktationsleistungen auf Testtagesmodelle umgestellt. Bei der Wurfgrösse beim Schwein oder anderen diskreten Merkmalen wurden auch `r r6objTableAbbrev$add_abbrev(psAbbrev = "GLMM", psMeaning = "Generalized Linear Mixed Models")` verwendet. Unabhängig von den verwendeten Modellen wurden in allen Auswertungen die gleichen Informationen berücksichtigt. 
- phänotypische Leistungen
- Pedigree
- Varianzkomponenten aus periodischen Schätzungen

Versuchsweise wurde ab den 1990-er Jahren erste genetische Marker mit in den Zuchtprogrammen berücksichtigt. Das Problem war dass diese wenigen Markern sehr schnell auf einer bestimmten Variante fixiert war. Nach der Fixierung lieferten diese Genorte keine zusätzliche Information zur Auswahl von potentiellen Zuchttieren. Es war zu dieser Zeit nicht klar, wie das Problem der Fixierung von einzelnen Genorten behandelt werden soll und es gab auch keine wirklich gute Strategie für die Berücksichtigung von genetischen Informationen in Zuchtprogrammen.


### Modellierung vor der genomischen Selektion
Vor der Einführung der genomischen Selektion war das BLUP-Tiermodell die Methode der Wahl für die Auswertung von Leistungsdaten in der Tierzucht. In seiner einfachsten Form sieht dieses Modell wie folgt aus.

\begin{equation}
  y = Xb + Zu + e
\end{equation}

\begin{tabular}{lll} 
wobei  &  $y$  &  Vektor mit phänotypischen Beobachtungen\\ 
       &  $b$  & Vektor mit fixen Effekten              \\ 
       &  $X$  & Inzidenzmatrix, welche fixe Effekte den Beobachtungen zuordnet\\ 
       &  $u$  & Vektor mit Zuchtwerten (zufällig) \\ 
       &  $Z$  & Inzidenzmatrix der Zuchtwerte \\ 
       &  $e$  & Vektor mit Residuen (zufällig) 
\end{tabular}

Die Co-Varianzen der zufälligen Komponenten sind definiert als:

$$Var(\mathbf{e}) = \mathbf{R} = \mathbf{I}*\sigma_e^2$$ 
$$Var(\mathbf{u}) = \mathbf{G} = \mathbf{A} * \sigma_g^2$$
$$Cov(\mathbf{u},\mathbf{e}^T) = Cov(\mathbf{e}, \mathbf{u}^T) = \mathbf{0}$$
$$\rightarrow Var(\mathbf{y}) = \mathbf{V} = \mathbf{ZGZ}^T + \mathbf{R}$$


## Genomische Selektion {#gensel}
Vom Standpunkt der Genetik aus basiert das BLUP-Tiermodell auf dem sogenannten Infinitesimalmodell. In diesem Modell wird angenommen, dass die phänotypische Ausprägung eines Merkmals durch die Summe von unendlich vielen Genorten mit undendlich kleiner Wirkung verursacht wird. Durch diese Annahme lässt sich dem einzelnen Tier kein fix definierter Genotyp mehr zuordnen. Diese fehlende Zuordnung der einzelnen Genotypen wird über die Modellierung der Zuchtwerte als zufällige Effekte gelöst. Die zufälligen Effekte der Zuchtwerte entsprechen dabei Realisierungen einer Zufallsvariablen mit vorgegebener Verteilung. 

In der genomischen Selektion verwenden wir das polygene Modell. Dabei werden die phänotypischen Leistungen als Summe von bekannten Genorten zusammengesetzt. Die konkrete Umsetzung des polygenen Modells wurde zum ersten Mal im Paper von [@MHG2001] gezeigt. Diese Autoren haben aufgrund von simulierten Daten gezeigt, dass es mit Hilfe einer sehr dichten Markerkarte möglich ist, die phänotypischen Leistungen alleine aufgrund der geschätzten Wirkungen an den Markergenorten zu modellieren. 

Die folgende Abbildung fasst die Unterschiede zwischen dem Infinitesimalmodell und dem polygenen Modell zusammen.

```{r AnimalModelVsGenomicSelection, conv.odg=TRUE, odg.path="odg", odg.graph.cache=TRUE,fig.align='center', echo=FALSE, results='asis'} 
knitr::include_graphics(path = "AnimalModelVsGenomicSelection.pdf")
```


### Modellierung
Im Zusammenhang mit der genomischen Selektion besteht die Modellierung der Daten aus zwei Komponenten

1. Die Schätzung der Gen-Wirkungseffekte ($a$)
2. Die Schätzung der genomischen Zuchtwerte

Die Umsetzung der beiden Komponenten wird in zwei verschiedenen Verfahren gemacht. Im Zwei-Schritt-Verfahren werden beide Komponenten einzeln an verschiedenen Teilen der Zuchtpopulation ausgeführt. Im Gegensatz dazu werden im Single-Step-Verfahren beide Komponenten im gleichen Schritt realisiert.


### Zwei-Schritt-Verfahren
Beim Zwei-Schritt-Verfahren wird die Population in ein Trainings- und ein Testset unterteilt. Im Trainingsset werden aufgrund von Typisierungsergebnissen und Beobachtungen die Gen-Wrkungseffekte ($a$) geschätzt. Sobald die Schätzwerte für die $a$-Effekte bekannt 
sind können diese für die Schätzung der genomischen Zuchtwerte verwendet werden.

Da aufgrund der Typisierungsergebnisse die Genotypen an den SNP-Genorten bekannt sind, brauchen wir kein gemischtes lineares Modell mehr. Im Gegensatz zur BLUP-Zuchtwertschätzung, ist in der genomischen Selektion beim Zwei-Schritt-Verfahren ein einfaches lineares Modell ausreichend. Im Idealfall, wenn die komplette Information zu allen Gen-Wirkungseffekten ($a$) bekannt sind, dann setzen sich die genotypischen Werte einfach zusammen aus den aufsummierten $a$-Werten. In Matrix-Vektor-Schreibweise können wir die folgende Modellgleichung aufstellen.

\begin{equation}
  g = 1\mu + Ma + \epsilon
\end{equation}

\begin{tabular}{lll}
wobei:  &  $g$         &  Vektor von wahren genomischen Zuchtwerten \\
        &  $\mu$       &  Achsenabschnitt \\
        &  $a$         &  Vektor mit Gensubstitutionseffekten \\
        &  $M$         &  Inzidenzmatrix als Verknüpfung zwischen $a$ und $g$ \\
        &  $\epsilon$  &  Vektor von zufälligen Residuen 
\end{tabular}

Die Matrix $M$ ist eine Inzidenzmatrix, welche die genotypischen Werte im Vektor $g$ mit den Gen-Wirkungseffekten $a$ verknüpft. Die Matrix $M$ hat die Dimension $n\times p$ wobei $n$ der Anzahl Individuen mit einem Typisierungsergebnis entspricht und $p$ gleich der Anzahl SNP-Genorte ist.

In der Realität im ersten Schritt des Zwei-Schritt-Verfahrens kennen wir aber weder die Komponenten des Vektors $g$ noch die Gensubstitutionseffekte $a$. Somit müssen wir das Modell zur Schätzung der $a$-Effekte modifizieren. Bei der aktuellen Modifikation ersetzen wir den Vektor $g$ durch die phänotypischen Beobachtung $y$. 

\begin{equation}
  y = (1\mu + Xb) + Ma + (\epsilon+ e)
\end{equation}  

\begin{tabular}{lll}
wobei:  &  $y$  &  Vektor der phänotypischen Beobachtungen \\
        &  $b$  &  Vektor der fixen Umweltfaktoren \\
        &  $X$  &  Inzidenzmatrix der fixen Effekte\\
        &  $e$  &  Vektor von nicht-genetische Residuen
\end{tabular}

Das Modell mit den phänotypischen Beobachtungen erlaubt eine Schätzung der $a$-Effekte. Mit diesem Ansatz gibt es aber zwei Probleme.

1. __Verfügbarkeit__: wirtschaftliche Merkmale wie Milchleistung sind nur beim weiblichen Geschlecht beobachtbar. Somit müsste für die Selektion auf der männlichen Seite wieder auf Nachkommenleistungen zurückgegriffen werden. Dies verlängert aber das Generationenintervall.
2. __Vergleichbarkeit__: Beim Austausch von Information zwischen verschiedenen Ländern sind die phänotypischen Leistungen nicht unbedingt vergleichbar.

Diese beiden Probleme können gelöst werden, wenn anstelle von phänotypischen Leistungen $y$, geschätzte Zuchtwerte $\hat{g}$ verwendet werden. Das entsprechende Modell sieht dann wie folgt aus. 

\begin{equation}
\hat{g} = g + (\hat{g} - g) = 1\mu + Ma + (\epsilon + (\hat{g} - g) )
\end{equation}

### Eigenschaften von BLUP-Zuchtwerten
Aufgrund der Eigenschaften von den BLUP-Zuchtwerten $\hat{g}$ führt die Addition der Abweichung $(\hat{g} - g)$ zu einer Reduktion der Varianz. Die Reduktion der Varianz bedeutet, dass $var(\hat{g}) \le var(g)$ ist. Für BLUP-Zuchtwerte gilt, dass die Covarianz zwischen wahrem und geschätztem Zuchtwert gleich der Varianz der geschätzten Zuchtwerte ist. In Formeln geschrieben bedeutet dass,

\begin{equation}
  cov(\hat{g},g) = var(\hat{g})
\end{equation}

Setzen wir diese Beziehung in die Varianz der Abweichung $(\hat{g} - g)$ ein, dann erhalten wir

\begin{equation}
  var(\hat{g} - g) = var(\hat{g}) + var(g) - 2cov(\hat{g},g) = var(g) - var(\hat{g}) \ge 0
\end{equation}

Somit gilt, dass $var(g) \ge var(\hat{g})$ und somit ist die Reduktion der Varianz gezeigt. Im Zusammenhang mit der Varianzreduktion steht auch die zweite Eigenschaft von BLUP-Zuchtwerten, welche uns hier Schwierigkeiten bereitet und zwar handelt es sich dabei um den sogenannten Shrinkage-Effekt. Für einen geschätzten Zuchtwert eines Tieres $i$ bedeutet das, dass dieser zum Durchschnitt der geschätzten Zuchtwerte der Eltern regressiert wird. Das Ausmass dieses Regressions-Effektes hängt davon ab, aufgrund welcher Informationen der Zuchtwert von Tier $i$ geschätzt wurde. Diese Abhängigkeit wird in der Zerlegung des geschätzten BLUP-Zuchtwertes des Tieres $i$ in seine Komponenten sichtbar. Diese Zerlegung ist in [@Hofer1990] und in [@vonRohr2016] erklärt. Das Resultat der Zerlegung ist in der nachfolgenden Formel zusammengefasst.

\begin{eqnarray}
\hat{g}_i &=& \frac{1}{1 + \alpha \delta^{(i)} + {\alpha\over 4} \sum_{j=1}^n \delta^{(k_j)}}
              \left[y_i - \hat{\mu} + {\alpha\over 2}\left\{\delta^{(i)}(\hat{g}_s + \hat{g}_d) 
              + \sum_{j=1}^n \delta^{(k_j)} (\hat{g}_{k_j} - {1\over 2}\hat{g}_{l_j}) \right\} \right]
\label{eq:AhatDecompEq}
\end{eqnarray}

Die Zerlegung des geschätzen Zuchtwertes $\hat{g}_i$ für Tier $i$ zeigt die Abhängigkeit des Ausmasses der Regression von $\hat{g}_i$ auf den Durchschnitt der geschätzten Elternzuchtwerte $\hat{g}_s$ und $\hat{g}_d$. Hat das Tier $i$ keine Eigenleistung $y_i$, keine Nachkommen und keine Paarungspartner, so ist $\hat{g}_i$ vollständig durch $\hat{g}_s$ und $\hat{g}_d$ bestimmt. Sobald aber Tier $i$ eine Eigenleistung hat und später dann noch Nachkommenleistungen dazukommen, nimmt der Einfluss von $\hat{g}_s$ und $\hat{g}_d$ auf $\hat{g}_i$ ab. Damit verringert sich auch das Ausmass des Regressions-Effektes von $\hat{g}_i$ auf den Durchschnitt der geschätzten Elternzuchtwerte.

Durch die Berücksichtigung zusätzlicher Informationen, wie Eigenleistung und Leistungen von Nachkommen und Paarungsparter, bei der Schätzung des Zuchtwertes für Tier $i$ steigt auch die Genauigkeit oder das Bestimmtheitsmass ($B$) des geschätzten Zuchtwertes. Wir können aufgrund der Eigenschaften von BLUP-Zuchtwerten können wir folgende Zusammenhänge aufstellen. Je grösser die verfügbare Information für die Schätzung eines Zuchtwertes für Tier $i$, desto grösser ist das Bestimmtheitsmass des geschätzten Zuchtwertes und je tiefer ist der Regressions-Effekt des geschätzten Zuchtwertes auf den Durchschnitt der geschätzten Zuchtwerte der Eltern und je geringer ist auch die Varianzreduktion.


### Einsatz von BLUP-Zuchtwerten in der genomischen Selektion
Eigenschaften von BLUP-Zuchtwerten führen zu Varianzreduktion und dazu dass geschätzte Zuchtwerte zum Durchschnitt der geschätzten Zuchtwerte der Eltern regressiert werden. Diese beiden Effekte sind problematisch bei der Verwendung von BLUP-Zuchtwerten für die Schätzung der $a$-Effekte in der genomischen Selektion. Ein bestimmtes Tier $i$ hat immer die gleichen SNP-Genotypen und wir gehen davon aus, dass diese auch immer die gleiche Wirkung auf die Ausprägung eines Phänotyps haben. Der mit BLUP geschätzte Zuchtwert eines Tieres ändert sich aber während seines Lebens. In der Zeitperiode der Geburt bis zur Beobachtung einer Eigenleistung ist der geschätzte Zuchtwert durch die geschätzten Zuchtwerte der Eltern bestimmt. Mit zunehmendem Alter werden für Tier $i$ mehr Informationen in der Zuchtwertschätzung berücksichtigt. Somit ändert sich der geschätzte Zuchtwert und damit würde sich auch die aufgrund der BLUP-Zuchtwerte geschätzten $a$-Effekte ändern. Das ist aufgrund von unserer Annahme der konstanten Wirkung der $a$-Effekte ein unerwünschtes Verhalten.

Die unerwünschten Veränderungen der geschätzten BLUP-Zuchtwerte werden durch eine Prozedur namens __Deregression__ korrigiert. Da sich die Veränderungen der Zuchtwerte im wesentlichen durch eine Funktion der Änderungen im Bestimmtheitsmass beschrieben werden kann, ist die Deregression als Korrektur von geschätzten Zuchtwerten aufgrund deren Bestimmtheitsmass definiert. Einzelheiten zur Deregression können dem Paper [@GTF2009] entnommen werden.


## Zusammenfassung
Die deregressierten Zuchtwerten werden als Beobachtunen für die Schätzung der $a$-Effekte im ersten Schritt des Zwei-Schritt-Verfahrens verwendet. Die geschätzen $a$-Werte werden dann verwendet um im zweiten Schritt die genomischen Zuchtwerte der restlichen Population zu berechnen. 

Die im Zwei-Schritt-Verfahren verwendeten Modelle zur Schätzung der $a$-Effekte sind einfache lineare Modelle. Die Anzahl der Parameter $p$ in diesen Modellen entspricht der Anzahl zu schätzender $a$-Werte und somit der Anzahl an SNPs pro Typisierung. Diese Anzahl ist typischerweise bei 50K kann aber auch bis 800K anwachsen. In den meisten Fällen ist $p >> n$, wenn $n$ die Anzahl typisierter Tiere ist. Somit können wir das klassische Least Squares Verfahren für die Schätztung der Parameter nicht verwenden.

## Ausblick
Das Problem $p >> n$ kommt heutzutage in sehr vielen Anwendungen vor. In den nachfolgenden Kapiteln wollen wir uns ein paar Lösungsansätze anschauen, welche uns trotz der spärlich verfügbaren Informationen in den hoch-dimensionalen Parameterräumen, sinnvolle Schätzwerte für die Parameter im Modell liefern kann.


<!--chapter:end:01-intro.Rmd-->

# Multiple Lineare Regression {#linreg}
Die multiple lineare Regression ist wie folgt definiert. Jedes Individuum oder jedes Objekt in einem Datensatz ist charakterisiert durch eine __Zielgrösse__ und einer Menge von __erklärenden Variablen__. Zusammengefasst besteht die bekannte Information für jedes Individuum oder jedes Objekt $i$ aus einem Datensatz aus der folgenden Menge 

$$\{x_{i,1}, x_{i,2}, \ldots, x_{i,p}, y_i\}$$
Das multiple lineare Regressionsmodell versucht die Zielgrösse bis auf einen zufälligen Restterm $\epsilon$ als lineare Funktion der erklärenden Variablen auszudrücken. Unser Ziel besteht in der Schätzung der unbekannten Parameter, welche im Regressionsmodell enthalten sind. Die nachfolgend gezeigte Modellformel soll die Unterscheidung zwischen erklärenden Variablen und unbekannten Parametern verdeutlichen.

\begin{equation}
y_i = \beta_i x_{i,1} + \ldots + \beta_p x_{i,p} + \epsilon_i \qquad (i = 1, \ldots, n)
\label{eq:MultLinRegForm}
\end{equation}

Fassen wir die Gleichungen über alle $(i = 1, \ldots, n)$ zusammen und verwenden die Matrix-Vektor-Notation, so sieht das lineare Modell in (\ref{eq:MultLinRegForm}) wie folgt aus.

\begin{equation}
y = X\beta + \epsilon
\label{eq:MultLinRegMatVec}
\end{equation}

\begin{tabular}{lll}
wobei  &              & \\
       &  $y$         &  Vektor der Länge $n$ mit allen Zielgrössen \\
       &  $\beta$     &  Vektor der Länge $p$ mit unbekannten Parametern \\
       &  $X$         &  Matrix der Dimension $n\times p$ mit erklärenden Variablen \\
       &  $\epsilon$  &  Vektor der Länge $n$ mit zufälligen Resteffekten
\end{tabular}

Die Reste $\epsilon_i$ im Modell (\ref{eq:MultLinRegForm}) haben wir als zufällige Effekte definiert. Somit müssen wir geeignete Annahmen zur Dichteverteilung der $\epsilon_i$ treffen. Meistens gehen wir davon aus, dass die $\epsilon_i$ unabhängig sind und der gleichen Verteilung folgen. In der englischsprachigen Literatur wird das mit dem Begriff `r r6objTableAbbrev$add_abbrev(psAbbrev = "i.i.d.", psMeaning = "independent, identically distributed")` bezeichnet. Der Erwartungswert und die Varianz der Zufallsvariablen $\epsilon$ sind $E\left[\epsilon_i \right] = 0$ und $Var(\epsilon_i) = \sigma^2$. 


## Beispiele für Lineare Regressionen
### Regression mit Achsenabschnitt
Die erste erklärende Variable wir oft als eine Konstante angenommen. Das bedeutet, dass der erste Kolonnenvektor in der Matrix $X$ gleich dem Eins-Vektor ist. Die konstante erklärende Variable erlaubt es einen sogenannten __Achsenabschnitt__ anzupassen. In skalarer Schreibweise hat das lineare Modell mit Achsenabschnitt die folgende Form

$$y_i = \beta_1 + \beta_2x_{i2} + \ldots + \beta_px_{ip} + \epsilon_i \qquad (i = 1,\ldots,n)$$

### Regression durch den Ursprung
Im Gegensatz zur Regression mit Achsenabschnitt steht die Regression durch den Ursprung. Diese kennt keine konstante erklärende Variable. Das Modell ohne Achsenabschnitt sieht dann wie folgt aus.

$$y_i = \beta_1x_{i1}+ \ldots + \beta_px_{ip} + \epsilon_i \qquad (i = 1,\ldots,n)$$

### Regression mit transformierten Variablen
Regressionen können auch auf Transformationen der erklärenden Variablen oder auf transformierte Zielgrössen angepasst werden. Als Beispiel verwendet die sogenannte "quadratische" Regression die $x_{ij}$ und die $x_{ij}^2$ als erklärende Variablen. Das Modell entspricht dann einer quadratischen Funktion in den $x_j$ ist aber immer noch eine lineare Funktion im Bezug auf die Parameter $\beta_j$.

$$y_i = \beta_1 + \beta_2 x_{i2} + \beta_3 x_{i2}^2 + \epsilon_i \qquad (i = 1,\ldots,n)$$

Abgesehen von der quadratischen Regression sind auch andere Arten von Transformationen der erklärenden Variablen denkbar. Ein Beipsiel ist in der folgenden Gleichung gezeigt. 

$$y_i = \beta_i + \beta_2 \log(x_{i2}) + \beta_3 sin(\pi x_{i3}) + \epsilon_i \qquad (i = 1,\ldots,n)$$

Auch dieses Modell ist _linear_ in den Parametern $\beta_j$ und wird somit als lineare Regression bezeichnet.

### Anwendungen in den Nutztierwissenschaften
Eine Anwendung der linearen Regression in den Nutztierwissenschaften ist die Schätzung vom Lebendgewicht von Tieren aufgrund des Brustumfangs. Dafür werden Messbänder verwendet, welche auf der einen Seite den Brustumfang angeben und auf der anderen Seite das geschätzte Körpergewicht. Diese Anwendung macht eine Voraussage der Zielgrösse `Körpergewicht` aufgrund der beobachteten erklärenden Variablen `Brustumfang`.

Damit eine Voraussage für die Zielgrösse aufgrund der erklärenden Variablen möglich ist, muss zuerst ein angemessener Datensatz vorliegen, in welchem man für jedes Tier beide Informationen, also sowohl Körpergewicht als auch Brustumfang bekannt ist. Aufgrund dieser Informationen können dann die unbekannten Parameter geschätzt werden. Die geschätzten Parameter werden dann für die Vorhersagen verwendet.

Bei diesem ersten Beispiel handelt es sich um eine einfache lineare Regression. Das verwendete Regressionsmodelle hat nur eine erklärende Variable (`Brustumfang`) und eine Zielvariable (`Gewicht`). Das zu dieser Anwendung zugehörige Modell lautet

$$y_{G,i} = \beta_1 + \beta_2 x_{B,i} + \epsilon_i$$

### Ziele der linearen Regression
- __Gute Anpassung__: das Modell soll so sein, dass die erklärenden Variablen möglichst präzise Voraussagen zu den Zielvariablen machen. Das Standardtool für die Anpassung ist die Methode der kleinsten Quadrate (`Least Squares`).

- __Parameterschätzung__: die unbekannten Parameter sollen so geschätzt sein, dass eine Veränderung der erklärenden Variablen in einer entsprechenden Veränderung der Zielgrösse führt.

- __Vorhersage__: noch nicht beobachtete Zielgrössen sollen als Funktionen von erklärenden Variablen vorhergesagt werden können

- __Fehler und Signigikanz__: werden durch Vertrauensintervalle und statistische Tests beurteilt

- __Modellentwicklung__: ist ein interaktiver Prozess, welche durch die oben genannten Ziele beeinflusst wird


## Methode der kleinsten Quadrate (Least Squares)
Gegeben sei das lineare Modell $y = X\beta + \epsilon$. Wir wollen eine, gemäss den oben formulierten Zielen, möglichst gute Schätzung für $\beta$ finden. Die folgende Darstellung erklärt, wie die Methode der kleinsten Quadrate funktioniert.

```{r LsqExplain, conv.odg=TRUE, odg.path="odg", odg.graph.cache=TRUE, fig.align='center', echo=FALSE, results='asis'}
knitr::include_graphics(path = "LsqExplain.pdf")
```

Die Punkte stehen für die Beobachtungen $y_i$. Die rote Linie steht für die Regressionsgerade. Die Distanz des Punktes zur Projektion in Richtung der $y$-Achse auf der Regressionslinie entspricht dem Residuum $r_i = y_i - x_i^T \hat{\beta}$. Für eine bestimmte Regressionsgerade (rote Linie im Diagramm) wird für jeden Punkt $y_i$ das entsprechende Residuum $r_i$ berechnet. Die Residuen $r_i$ werden quadriert und addiert. Diese summierten Quadrate der Residuen stellt ein Mass dar, wie gut die Regressionsgerade an die Beobachtungspunkte $y_i$ angepasst ist. 

Position und Verlauf der Regressionsgeraden können durch die Wahl des Vektors $\beta$ beeinflusst werden. Gemäss der Methode der kleinsten Quadrate soll $\beta$ so bestimmt werden, dass die Summe der quadrierten Residuen minimal wird. Der so bestimmte Vektor $\beta$ wird dann als Least-Squares-Schätzer bezeichnet. In einer Formel können wir die Berechnung des Least-Squares-Schätzers ($\hat{\beta}$), wie folgt ausdrücken. 

$$\hat{\beta} = argmin_{\beta} \| y - X\beta \| ^2$$

wobei $\| .\|$ für die Euklidsche Norm oder die Euklidsche Distanz steht. In einem ersten Schritt geht es darum das Minimum für den Ausdruck $\| y - X\beta \| ^2$ zu finden. Dabei ist es einfacher, wenn wir folgende Umformung verwenden.

$$\| y - X\beta \| ^2 = (y - X\beta)^T(y - X\beta) = y^Ty - y^TX\beta - \beta^TX^Ty + \beta^TX^TX\beta$$

Leiten wir diesen Ausdruck nach $\beta$ ab und setzen die erste Ableitung gleich $0$, dann erhalten wir eine Gleichung für den Least-Squares-Schätzer $\hat{\beta}$.

$$-y^TX - y^TX + 2\hat{\beta}^TX^TX = 0$$

Aus der obigen Formel können wir die sogenannte __Normalgleichung__ herleiten. Diese lautet 

$$X^TX\hat{\beta} = X^Ty$$

Unter der Annahme, dass die Matrix $X$ vollen Kolonnenrang $p$ hat, können wir explizit nach $\hat{\beta}$ auflösen.

$$\hat{\beta} = (X^TX)^{-1}X^Ty$$

Die Residuen $r_i = y_i - x_i^T\hat{\beta}$ sind Schätzungen für die Resteffekte $\epsilon_i$ und können somit für die Schätzung von $\sigma^2$ verwendet werden. 

$$\hat{\sigma}^2 = \frac{1}{n-p}\sum_{i=1}^{n} r_i^2$$

Der Faktor $1/(n-p)$ scheint ungewöhnlich, aber es kann gezeigt werden, dass die Wahl dieses Faktors zur Erwartungstreue von $\hat{\sigma}^2$ führt. Das heisst, es gilt $E\left[ \hat{\sigma}^2 \right] = \sigma^2$. 


### Annahmen hinter dem linearen Modell
Abgesehen davon, dass die Matrix $X$ vollen Kolonnenrang $p<n$ haben muss, wurden für die erklärenden Variablen keine Annahmen getroffen. Insbesondere können die erklärenden Variablen kontinuierlich oder diskret sein. Kontinuierliche Variablen sind typischerweise Messgrössen, welche als reelle Zahlen (Gleitkommazahlen) erhoben werden. Diskrete Grössen können nur bestimmte Werte, wie zum Beispiel $0$ oder $1$ annehmen.

Damit die Anpassung eines linearen Modells mit der Methode der kleinsten Quadrate Sinn macht und die Tests und Vertrauensintervalle gültig sind, müssen wir gewisse Annahmen treffen.

1. __Korrektheit des linearen Modells__: Das heisst $E\left[\epsilon_i \right] = 0$ für alle $i$. Das heisst aber auch, dass die Zielgrössen und die erklärenden Variablen nicht gemischt werden dürfen.
2. __Alle $x_i$ sind exakt__: Es wird angenommen, dass die Werte für $x_i$ ohne Fehler beobachtet werden können.
3. __Konstante Varianz der Resteffekte__: $Var(\epsilon_i) = \sigma^2$ für alle $i$
4. __Resteffekte sind unkorreliert__: $Cov(\epsilon_i, \epsilon_j) = 0$ für alle $i\ne j$
5. __Resteffekte folgen Normalverteilung__: Der Vektor $\epsilon$ der Resteffekte folgt einer multivariaten Normalverteilung.

Falls diese Annahmen verletzt sind, gibt es eine Reihe von Massnahmen, welche getroffen werden können. Bei Verletzung der Annahme 3, können "weighted least squares" Methoden verwendet werden. Ähnlich bei Verletzung der Annahme 4, können wir "generalized least squares" verwenden. Ist die Annahme 5 der Normalverteilung nicht erfüllt, können wir auf sogenannte "robuste Methoden" ausweichen. Falls Annahme 2 nicht zutrifft, braucht es Korrekturen, welche als "errors in variables" bezeichnet wird. Falls die Annahme 1 nicht stimmt, braucht es nicht-lineare Modelle.

Die folgende Grafik zeigt das Beispiel des sogenannten "Pillen-Knicks". Dabei werden die Anzahl Geburten seit 1930 in der Schweiz gezeigt. Hier sind die Annahmen 1 und 4 verletzt. Dieses Beispiel zeigt auch die Gefahr bei Vorhersagen in Bereiche, wo keine erklärende Variablen vorliegen.

```{r PillKink, conv.odg=TRUE, odg.path="odg", odg.graph.cache=TRUE, fig.align='center', echo=FALSE, results='asis'}
knitr::include_graphics(path = "PillKink.pdf")
```


### Kein Ersatz der multiplen Regression durch mehrere einfache Regressionen
Eine multiple Regression (mit mehreren erklärenden Variablen) soll nicht durch mehrere einfache Regressionen (mit nur einer erklärenden Variablen) ersetzt werden. Das folgende simulierte Beispiel zeigt weshalb.

Wir betrachten die folgenden erklärenden Variablen $x^{(1)}$ und $x^{(2)}$ und die Zielgrösse $y$ mit folgenden Werten

```{r MultRegXval, echo=FALSE, results='asis'}
x1 <- c(0, 1, 2, 3, 0, 1, 2, 3)
x2 <- c(-1, 0, 1, 2, 1, 2, 3, 4)
y <- 2*x1-x2
knitr::kable(data.frame(x1,x2,y))
```

Die multiple Regression führt zur Lösung der kleinsten Quadrate, welche die Daten exakt beschreibt, so wie diese erzeugt wurden.

$$y_i = \hat{y_i} = 2x_{i1} - x_{i2} \qquad \text{für alle } i \text{ mit } \hat{\sigma}^2 = 0$$

Wird an die Daten nur eine einfache Regression mit der erklärenden Variablen $x^{(2)}$ und ignoriert $x^{(1)}$, so erhalten wir das folgende Resultat

$$\hat{y_i} = {1\over 9}x_{i2} + {4\over 3} \qquad \text{für alle } i \text{ mit } \hat{\sigma}^2 = 1.72$$

Der Grund dafür ist, dass die erklärenden Variablen $x^{(1)}$ und $x^{(2)}$ korreliert sind. Falls $x^{(1)}$ steigt, dann steigt auch $x^{(2)}$. Da aber in der multiplen Regression $x^{(1)}$ einen grösseren Koeffizienten hat als $x^{(2)}$, muss dieser Effekt in der einfachen Regression durch $x^{(2)}$ kompensiert werden. Dies führt zur Abweichung zwischen den Resultaten der beiden Analysen.


## Eigenschaften der Schätzungen
Die Least-Squares-Schätzer sind Zufallsvariablen, da für jeden Datensatz den wir vom gleichen unterliegenden Prozess beobachten, andere Werte resultieren. Damit ändern sich auch die Least-Squares-Schätzer. Da die Schätzer Funktionen der beobachteten Daten sind, haben die Schätzer auch einen zufälligen Charakter. Somit können wir Eigenschaften betreffend den Verteilungen und den Momenten für die Least-Squares-Schätzer herleiten. Die Ergebnisse sind hier nur kurz zusammengefasst.

### Momente der Least-Squares Schätzungen
Wir nehmen das folgende lineare Modell an

$$y = X\beta + \epsilon \text{, } E\left[\epsilon \right] = 0 \text{, } Cov(\epsilon) = E\left[\epsilon\epsilon^T \right] = \sigma^2I_{n\times n}$$

Zusammen mit den oben getroffenen Annahmen können wir folgende Aussagen machen

1. $E\left[\hat{\beta}\right] = \beta$, das heisst, $\hat{\beta}$ ist unverzerrt
2. $E\left[\hat{y}\right] = E\left[y\right] = X\beta$, was aus 1. folgt. Zudem ist, $E\left[r\right] = 0$
3. $Cov(\hat{\beta}) = \sigma^2(X^TX)^{-1}$
4. $Cov(\hat{y}) = \sigma^2P$, $Cov(r) = \sigma^2(I-P)$

Die Matrix $P$ ist definiert als Projektionsmatrix aus $\hat{y} = Py$. Setzen wir die Least-Squares-Schätzer ein, dann folgt

$$\hat{y} = X\hat{\beta} = X(X^TX)^{-1}X^Ty = Py$$

Somit ist die Matrix $P$ definiert als $P=X(X^TX)^{-1}X^T$. 

### Verteilung der Least-Squares-Schätzer unter normalverteilten Fehlern
Zusätzlich zum linearen Modell nehmen wir an, dass $\epsilon_i, \ldots, \epsilon_n \text{ i.i.d. } \mathcal{N}(0,\sigma^2)$, dann können wir zeigen, dass

1. $\hat{\beta} \sim \mathcal{N}_p\left(\beta, \sigma^2(X^TX)^{-1}\right)$
2. $\hat{y} \sim \mathcal{N}_n\left(X\beta,\sigma^2P \right)$, $r \sim \mathcal{N}_n\left(0,\sigma^2(I-P) \right)$
3. $\hat{\sigma}^2 \sim \frac{n}{n-p}\chi_{n-p}^2$

Die Annahme der Normalverteilung ist oft (annähernd) erfüllt und kann durch den zentralen Grenzwertsatz bei grösseren Datensätzen begründet werden. Diese Eigenschaften im Bezug auf die Verteilung der Schätzer führt zur Herleitung von Vertrauensintervallen und statistischen Tests für die geschätzten Parameter. Sind die Annahmen der Normalverteilung nicht erfüllt, müssen wir auf sogenannte robuste Methoden ausweichen. Diese werden hier nicht behandelt.


## Tests und Vertrauensintervalle
### Einzeltests
Wir nehmen an, dass das lineare Modell korrekt ist und dass die Resteffekte $\epsilon_1, \ldots, \epsilon_n \text{ i.i.d. } \sim \mathcal{N}\left(0, \sigma^2 \right)$. Dann haben wir gesehen gemäss den Eigenschaften aus dem vorherigen Abschnitt ist dann $\hat{\beta}$ normalverteilt.

Im Allgemeinen sind wir daran interessiert, ob ein bestimmter Parameter $\beta_j$ einen Einfluss hat. Dies lässt sich mit der Nullhypothese $H_{0,j}: \beta_j = 0$ gegenüber der Alternativen $H_{A,j}: \beta_j \ne 0$ überprüfen. Da $\hat{\beta}$ einer Normalverteilung folgt, können wir herleiten, dass unter der Nullhypothese $H_{0,j}$ gilt

$$\frac{\hat{\beta_j}}{\sqrt{\sigma^2(X^TX)_{jj}^{-1}}} \sim \mathcal{N}(0,1)$$

Da $\sigma^2$ unbekannt ist, ist die obige Teststatistk in der Praxis nicht brauchbar. Ersetzen wir $\sigma^2$ durch den Schätzwert $\hat{\sigma}^2$ so erhalten wir die sogenannte t-Teststatistik.

$$T_j = \frac{\hat{\beta_j}}{\sqrt{\hat{\sigma}^2(X^TX)_{jj}^{-1}}} \sim t_{n-p}$$

Anhand dieses Tests können wir die Relevanz der erklärenden Variablen quantifizieren, indem wir die Teststatistiken $T_j$ für $j=1,\ldots,p$ analysieren. Die Beurteilung der Relevanz der erklärenden Variablen aufgrund dieser einzelnen t-Tests birgt zwei Probleme.

1. __Multiples Testen__: Werden sehr viele Tests durchgeführt, dann sind bei einem angenommenen Signifikanz-Niveau von $\alpha$ automatisch ein Anteil $\alpha$ aller Tests signifikant. Werden beispielsweise $100$ Tests auf dem Niveau $\alpha = 0.05$ durchgeführt, dann sind automatisch $5$ Tests signifikant.
2. __Korrelation der erklärenden Variablen__: Falls die erklärenden Variablen untereinander korreliert sind, dann beeinflusst dies auch die Testergebnisse und kann diese verzerren.

### Globaler Test
Wenn wir testen wollen, ob (abgesehen vom Achsenabschnitt) überhaupt eine erklärende Variable einen Einfluss auf die Zielgrösse hat, dann können wir diese mit folgender Nullhypothese $H_0: \beta_2 = \ldots = \beta_p = 0$ versus die Alternative $H_A: \beta_j \ne 0$ für $j=2,\ldots, p$ tun. Solch ein Test kann mit der Zerlegung der Varianz der Beobachtungen $y_i$ um das globale Mittel $\bar{y} = n^{-1}\sum_{i=1}^ny_i$ konstruiert werden. In Vektor-Schreibweise sieht diese Zerlegung wie folgt aus

$$\|y - \bar{y}\|^2 = \|\hat{y} - \bar{y}\|^2 + \|y - \hat{y}\|^2$$

Diese Zerlegung teilt die quadrierten Abweichungen der Beobachtungen $y$ vom allgemeinen Mittel $\bar{y}$ in die quadrierten Abweichungen der gefitteten Werte $\hat{y}$ vom allgemeinen Mittel plus die quadrierten Residuen $y-\hat{y}$ auf. Eine solche Zerlegung lässt sich am einfachsten in einer Varianzanalysetabelle zusammenfassen.

\begin{tabular}{llll}
\hline
            &   Summenquadrate             &  Freiheitsgrade &  mittlere Summenquadrate          \\
\hline
Regression  &   $\|\hat{y} - \bar{y}\|^2$  &  $p-1$          &  $\|\hat{y} - \bar{y}\|^2/(p-1)$  \\
Rest        &   $\|y - \hat{y}\|^2$        &  $n-p$          &  $\|y - \hat{y}\|^2/(n-p)$        \\
\hline
Total       &   $\|y - \bar{y}\|^2$        &  $n-1$          &
\end{tabular}

Im Falle der globalen Nullhypothese haben die erklärenden Variablen keinen Einfluss auf die Zielgrösse. Somit ist $E\left[y \right] = const. = E\left[\bar{y}\right]$. Daraus folgt, dass der Erwartungswert der mittleren Summenquadrate der Regression gleich $\sigma^2$ ist. Teilen wir die mittleren Summenquadrate der Regression durch die mittleren Summenquadrate des Rests (Schätzung von $\sigma^2$) erhalten wir eine dimensionslose Grösse, welcher einer $F$-Statistik entspricht. Unter der Nullhypothese gilt, dass 

$$F = \frac{\|\hat{y} - \bar{y}\|^2/(p-1)}{\|y - \hat{y}\|^2/(n-p)} = F_{p-1,n-p}$$

Dies wird als globaler $F$-Test der Regression bezeichnet.

Abgesehen von der Bewertung der statistischen Signifikanz mit dem globalen $F$-Test, sind wir auch daran interessiert, wie gut die Anpassung des Modells an die Daten ist. Eine mögliche Grösse für die Qualität der Anpassung ist das sogenannten $R^2$. Dies ist definiert als das folgende Verhältnis.

$$R^2 = \frac{\|\hat{y} - \bar{y}\|^2}{\|y - \bar{y}\|^2}$$

Das $R^2$ entspricht dem Verhältnis der Variation der Beobachtungen um das globale Mittel, welcher durch die Regression erklärt werden kann. Aus dieser Definition ist klar, dass wir nach Modellen suchen mit einem möglichst grossen $R^2$. 

### Vertrauensintervalle
In Anlehnung an den $t$-Test der einzelnen Parameter $\beta_j$ können wir Vertrauensintervalle ableiten. Das zwei-seitige Vertraunensintervall auf dem Niveau $1-\alpha$ für $\beta_j$ ist definiert als

$$\hat{\beta}_j \pm \sqrt{\hat{\sigma}^2(X^TX)_{jj}^{-1}} \ \cdot t_{n-p;1-\alpha/2}$$

Hier $t_{n-p;1-\alpha/2}$ ist das $1-\alpha/2$-Quantil der $t_{n-p}$-Verteilung.


## Output von R
In R wird eine lineare Regression mit der Funktion `lm()` angepasst. Die Zusammenfassung der Resultate von `lm()` ist in der nachfolgenden Diagramm gezeigt.

```{r LinModResults, conv.odg=TRUE, odg.path="odg", odg.graph.cache=TRUE, fig.align='center', echo=FALSE, results='asis'}
knitr::include_graphics(path = "LinModResults")
```

Die verschiedenen Bereiche der Resultate sind durch farbige Rechtecke gekennzeichnet.


<!--chapter:end:02-linreg.Rmd-->

<!-- ------------------------------------------------------------------- --
  -- END of document:  Below this must not be anything, except the stuff --
  -- concerning the table of abbreviations                               --> 
  

```{r WriteTableOfAbbreviations, echo=FALSE, results='hide'}
if (!r6objTableAbbrev$is_empty_abbr())
  r6objTableAbbrev$writeToTsvFile()
```

```{r AbrTableToDocument, echo=FALSE, results='asis'}
r6objTableAbbrev$include_abbr_table(psAbbrTitle = "# Abkürzungen {-}")
```


```{r Bibliography, echo=FALSE, results='hide'}
bref <- c(bibentry(
     bibtype = "Article",
     title = "Prediction of total genetic value using genome-wide dense marker maps",
     author = c(as.person("Theo HE Meuwissen [aut]"), 
                as.person("Ben J Hayes [aut]]"),
                as.person("Mike E Goddard [aut]")),
     year = "2001",
     journal = "Genetics",
     number = "157",
     pages = "1819-1829",
     key = "MHG2001"),
  bibentry(
    bibtype = "Book",
    title = "Handbook of Statistical Genetics",
    editor = c(as.person("D. J. Balding"), as.person("M. Bishop"), as.person("C. Cannings")),
    year = "2009",
    publisher = "Wiley",
    doi = "10.1002/9780470061619",
    key = "BBC2008"
  ),
  bibentry(
    bibtype = "PhdThesis",
    title = "Schätzung von Zuchtwerten feldgeprüfter Schweine mit einem Mehrmerkmals-Tiermodell",
    author = as.person("A. Hofer [aut]"),
    year = "1990",
    school = "ETH Zürich",
    key = "Hofer1990"
  ),
  bibentry(
    bibtype = "Unpublished",
    title = "Züchtungslehre",
    author = as.person("P. von Rohr [aut]"),
    year = "2016",
    note = "Vorlesungsunterlagen ETHZ, HS2016",
    url = "https://charlotte-ngs.github.io/LBGHS2016",
    key = "vonRohr2016"
  ),
  bibentry(
    bibtype = "Article",
    title   = "Deregressing Estimated Breeding Values and Weighting Information for Genomic Regression Analyses",
    author  = c(as.person("D. Garrick"), 
                as.person("J. Taylor"), 
                as.person("R. Fernando")),
    year    = "2009",
    journal = "Genetics Selection Evolution",
     number = "41(1)",
     pages = "55",
     key = "GTF2009"
  )
)


### # Fixed assignmen of bib file
sBibFile <- "ASMNW.bib"
if(!file.exists(sBibFile))
  cat(paste(toBibtex(bref), collapse = "\n"), "\n", file = sBibFile)
```



`r if (knitr:::is_html_output()) '# References {-}'`

<!--chapter:end:99-abbrevref.Rmd-->

